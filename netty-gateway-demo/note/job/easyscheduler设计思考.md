## 扩展和升级方案
### 任务分配优化
问题：
- 任务分片
  - 简单分片：task需要增加字段，分片规则，分片数量（需要业务指定），
    分片编号 （对应work节点启动注册后按顺序自动分配的编号，work编号也可以指定）
  - 自动分片：分片规则（如资源优先，按照工作组的work节点资源比例动态分片;
    如权重优先，则根据work节点权重进行分片），分片数量
  
- 负载均衡
  master这边是简单的单线程轮询DAG的任务链，依次执行DAG的task，线程等待时间较长，浪费cpu资源
  work节点也是无脑抓取批量任务多线程执行，分布式锁限制任务并行抓取，无脑抓取也导致系统资源使用不均衡

为提高master和work吞吐量，可以引入消息队列。

比如任务链 s->a，a->b,a->c,b->d,c->d,a的执行只依赖s,但d的执行依赖b和c的结果。

DAG优化执行方案如下：
1. master解析DAG1，将s,a,b,c,d 生成taska,b,c,d定义入库(task a定义包括前置依赖s,后续c、d，所属processDef),
   发送DagEvent(Dag1)到消息队列。 
2. master收到DagEvent(Dag1),开始执行Dag,分析生成首个task s。
   判断task a是否需要分片：
   - 需要分片
     根据分片信息（分片数量，分片规则）生成子task， 
     对task执行进行work节点负载均衡（采用工作组_加编号N形式，不采用Ip是为了work节点的扩展性），标记执行节点编号 
     子task记录task及分片信息,持久化入库，标记开始执行状态，并**发送TaskExeEvent(task_s_N,N{1...N})到工作组广播队列**
   - 不需要分片 
     直接生成TaskS, 并**发送TaskExeEvent(task_s)到工作组订阅队列**
    

3. work监听广播队列和订阅队列的TaskExeEvent消息，分两种模式执行。
   - task是分片任务，则是监听广播队列，监听TaskExeEvent(task_a_N),判断编号是否和自己匹配，匹配则进行执行
   - task是正常任务，则是监听的订阅队列，任务直接执行
     
    执行完s后修改Task s状态持久化入库，标记执行机器信息，并发送TaskCompleteEvent(s->a)，
   
4. master 监听到TaskCompleteEvent(s->a),可以判断a的执行条件是否达成（数据库检测s的状态是否完成，a是否有其他前置条件）
   如果a执行条件达成，则发送TaskPreEnQueueEvent(a)至消息队列。
   
5. master监听TaskPreEnQueueEvent(a),同步骤2处理。任务推送队列 TaskExeEvent(task_a_N)
6. work监听TaskExeEvent(task_a_N),判断编号是否和自己匹配，匹配则进行执行，
   work执行完task_a_N对a任务进行判断，是否分片均执行完毕？执行完毕则发送TaskCompleteEvent(a->b,a->c)
7. b,c执行循环2-6，最后完成是发送TaskCompleteEvent(b->d,c->d)
8. 无论TaskCompleteEvent(b->d,c->d)先后，消息都判断d的前置执行条件是否满足
     （前置条件是b,c的task状态均成功完成或者自定义条件）
   
9. 最后d执行完，task d定义带有Dag标识，发送DagCompletedEvent消息队列，master监听消息后完成处理

该方案适合任务量极多的业务场景，但缺陷是每个消息可能都带有一次数据库查询来确认任务状态，
针对这种对数据的压力，可以采用分库分表的策略，根据工作组维度进行分库分表。

### 任务执行优化
- 失败任务判定
  task定义超时时间，master定期轮询task Instance表查找运行中状态切超时时间小于当前时间的任务。
  找到则重新构建task_s_N消息（多设置重试编号字段，0默认是没有重试的），让其他work节点执行
- 失败任务重试
  每次执行会增加task Instance的执行次数，如果超过执行次数，则根据定义策略拒绝服务或者进行告警
  
实现方案：
master节点争取分布锁，抢到则进行漏调度扫描，超时任务扫描。
重试、失败转移执行过程类似，根据任务类型不同重新构造，扔到广播队列或者订阅队列



### 架构优化
引入消息队列后，加入系统复杂度，而zk作用弱化为注册中心。
可以考虑引入etcd取代mq+zk的作用
- etcd watch目录机制实现订阅队列、广播队列
- etcd ttl机制监听注册节点失效，实现注册中心，提供负载均衡，分片基础
- etcd raft kv存储 各节点上报状态，加TTL方式删除失效节点资源信息，提供负载均衡，分片基础

### 客户端优化
新增scheduler client组件，和任务类型，可以以sdk的方式供业务使用

- 增加改客户端可让用户专注业务实现，提供统一接口
- 构建统一交换报文，提供任务链扩展。比如任务执行完可以传递结果给下一个任务

实现方案流程：
1. work节点执行任务触发api请求到sdk,将任务状态设置为执行中
2. sdk客户端执行完毕，回调请求到api组件，api组件落库成功后并发送消息到mq
3. master订阅执行完毕消息

### 多环境迁移
导入导出功能勉强可以满足多环境迁移的需求，不过毕竟是需要手动操作，容易误操作。

优化方案：
对api组件进行优化，添加环境迁移功能。api仅仅对接界面操作，资源需要少，无状态，
特别适合做跨多环境的桥梁。api可以监控三个环境的任何信息（job运行情况，zk，mq,数据库等信息）

环境迁移需要考虑以下信息：
- 工作流定义本身
- 工作流所属的项目，用户，租户

这些信息可能在不同环境冲突，可以考虑设计之初就是不用环境共用一套信息
  角色（1：n）用户(1:n)项目(1:n)工作流定义

共用信息包括角色，用户，项目。工作流定义这类信息根据环境可不同

为了兼容迁移的规则，需要对用户，项目，租户进行权限管理，并根据环境分配不同权限。
可以参考apollo的权限控制

### 监控优化
#### 调用链优化
集成skywalking,自定义探针，监控任务链路和执行情况
#### metrics 优化
集成grafana,prometheus，监控master,work 任务数，cpu,io等指标信息

### 第三方优化
第三方业务需要发布定时任务，可能是不通过界面操作的方式。
需要提供api接口实现任务发布，取消的接口。

任务发布需要包括以下信息：
- 触发规则，比如corn
- 工作流定义（包含执行规则，dag,是否分片，依赖关系）

虽热是第三发业务需求，管理员同样需要在页面管理。
那么这些通过api生成的工作流定义同样会挂在特定用户，项目，租户下面


### 总结
完成以上优化即可支持以下特性：
- 分布式调度协调
- 弹性扩容缩容
- 失效转移
- 错过执行作业重触发
- 作业分片一致性，保证同一分片在分布式环境中仅一个执行实例
- 自诊断并修复分布式不稳定造成的问题
- 支持并行调度
- 支持作业生命周期操作
- 丰富的作业类型
- Spring整合以及命名空间提供
- 运维平台